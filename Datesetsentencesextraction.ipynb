{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "049157a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582a1b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\envs\\AI_ENV\\lib\\site-packages\\spacy\\language.py:2195: FutureWarning: Possible set union at position 6328\n",
      "  deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 277750 sentences, 62009 marked as dataset-related.\n"
     ]
    }
   ],
   "source": [
    "# ✅ Fast Dataset Mention Extraction (Regex + Keywords + Small SpaCy)\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# --- Paths ---\n",
    "XML_DIR = r\"make-data-count-finding-data-references\\train\\train_XML\\XML\"\n",
    "\n",
    "# --- Load smaller NLP model ---\n",
    "\n",
    "nlp_scispacy = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# --- Regex patterns for dataset IDs ---\n",
    "DATASET_PATTERNS = [\n",
    "    r'10\\.\\d{4,9}/[-._;()/:a-z0-9]+',       # DOIs\n",
    "    r'doi:10\\.\\d{4,9}/[-._;()/:a-z0-9]+',\n",
    "    r'gse\\d{4,6}', r'srp\\d+', r'empiar-?\\d+', r'ens[a-z]{0,5}\\d+',\n",
    "    r'nm_\\d+', r'bx\\d+', r'cp\\d+', r'sth\\d+', r'f\\d+[a-z]+\\d*',\n",
    "    r'rs\\d+', r'hgnc:\\d+', r'cab\\d+', r'hpa\\d+', r'[a-z]\\d{5}',\n",
    "    r'epi_isl_\\d+', r'k\\d+', r'cvcl_\\d+', r'e-prot-\\d+',\n",
    "    r'pxd\\d+', r'prjna\\d+', r'srx\\d+', r'ku\\d+', r'e-geod-\\d+',\n",
    "    r'\\d{1}[a-z0-9]{3}', r'\\d+\\.\\d+\\.\\d+\\.\\d+', r'model\\d+', r'err\\d+', r'srr\\d+'\n",
    "]\n",
    "regex_combined = re.compile(\"|\".join(DATASET_PATTERNS), re.IGNORECASE)\n",
    "\n",
    "# --- Keywords for dataset mentions ---\n",
    "DATASET_KEYWORDS = [\n",
    "    \"data available\", \"dataset\", \"accession number\", \"repository\",\n",
    "    \"dryad\", \"pdb\", \"geo\", \"arrayexpress\", \"figshare\", \"ebi\",\n",
    "    \"sequence read archive\", \"chembl\", \"mgnify\"\n",
    "]\n",
    "\n",
    "def is_dataset_like(sentence):\n",
    "    \"\"\"\n",
    "    Check if a sentence is likely dataset-related.\n",
    "    \"\"\"\n",
    "    if regex_combined.search(sentence.lower()):\n",
    "        return True\n",
    "    for kw in DATASET_KEYWORDS:\n",
    "        if kw in sentence.lower():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# --- Extract sentences and identifiers from XML ---\n",
    "def extract_sentences_from_xml(xml_path):\n",
    "    with open(xml_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"lxml-xml\")\n",
    "\n",
    "    text_blocks = [tag.get_text(separator=\" \", strip=True) \n",
    "                   for tag in soup.find_all([\"p\", \"sec\", \"title\", \"abstract\", \"ref\", \"supplementary-material\"])]\n",
    "    sentences = [sent for block in text_blocks for sent in sent_tokenize(block)]\n",
    "\n",
    "    # Add all <idno> tags (DOI, PDB, dataset IDs, etc.) as pseudo-sentences\n",
    "    idno_tags = soup.find_all(\"idno\")\n",
    "    for tag in idno_tags:\n",
    "        id_type = tag.get(\"type\", \"unknown\").upper()\n",
    "        value = tag.get_text(strip=True)\n",
    "        if value:\n",
    "            sentences.append(f\"Referenced dataset {id_type}: {value}\")\n",
    "\n",
    "    return sentences\n",
    "\n",
    "# --- Process all XML files ---\n",
    "all_results = []\n",
    "xml_files = [os.path.join(XML_DIR, f) for f in os.listdir(XML_DIR) if f.endswith(\".xml\")]\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    article_id = os.path.basename(xml_file).replace(\".xml\", \"\")\n",
    "    sentences = extract_sentences_from_xml(xml_file)\n",
    "\n",
    "    for sent in sentences:\n",
    "        dataset_flag = is_dataset_like(sent)\n",
    "        all_results.append({\n",
    "            \"article_id\": article_id,\n",
    "            \"sentence\": sent,\n",
    "            \"is_dataset_like\": int(dataset_flag)\n",
    "        })\n",
    "\n",
    "# --- Save output ---\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(\"fast_dataset_sentences.csv\", index=False)\n",
    "print(f\"✅ Extracted {len(df_results)} sentences, {df_results['is_dataset_like'].sum()} marked as dataset-related.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5395946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36aa412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
